{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDbZ8UuImsqfB2M9Tz5ocv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Christian-Stefan/Team-Internship/blob/Chris/ContextModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fusion Model"
      ],
      "metadata": {
        "id": "8HaB8B84Yo7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context Branch"
      ],
      "metadata": {
        "id": "gk1152VTYsrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContextBranch(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__() # Constructor to initialize the `upper` constructor/pervious constructor? Why?\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv3d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(2),  # (7,256,256) → (3,128,128)\n",
        "\n",
        "            nn.Conv3d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool3d((1, 1, 1))  # output: [B, 32, 1, 1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x.view(x.size(0), -1)  # [B, 32]"
      ],
      "metadata": {
        "id": "nJlr8m0zYrN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Radiomics Branch"
      ],
      "metadata": {
        "id": "i4x0CYiUYxmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RadiomicsBranch(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "h9UGLYtZYzba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Branch"
      ],
      "metadata": {
        "id": "vX1QCJ1WY05i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LocalBranch(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv3d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(2),  # (5,64,64) → (2,32,32)\n",
        "\n",
        "            nn.Conv3d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool3d((1, 1, 1))  # output: [B, 32, 1, 1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x.view(x.size(0), -1)  # flatten to [B, 32]"
      ],
      "metadata": {
        "id": "Qt_Mq4TrY26T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Header"
      ],
      "metadata": {
        "id": "jafQSVIMfP18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TripleFusionModel(nn.Module):\n",
        "    def __init__(self, num_classes, radiomics_dim=25):\n",
        "        self.local_branch = LocalBranch()\n",
        "        self.context_branch = ContextBranch()\n",
        "        self.radiomics_branch = RadiomicsBranch(radiomics_dim)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(32 + 32 + 32, 64),  # fuse outputs\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, volume_local, volume_context, radiomics):\n",
        "        local_feat = self.local_branch(volume_local)\n",
        "        context_feat = self.context_branch(volume_context)\n",
        "        radio_feat = self.radiomics_branch(radiomics)\n",
        "\n",
        "        fused = torch.cat([local_feat, context_feat, radio_feat], dim=1)\n",
        "        out = self.classifier(fused)\n",
        "        return out"
      ],
      "metadata": {
        "id": "yro6emuPfRws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Selection"
      ],
      "metadata": {
        "id": "FOTBsvhVZ5ZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Particle Swarm Optimization (PSO) - Hyperparameter Optimization Initialization & Explanation\n",
        "\n",
        "In Particle Swarm Optimization (PSO), **particles** represent candidate solutions—for example, a randomly initialized set of hyperparameters for a machine learning model.\n",
        "\n",
        "- Each particle moves within a **D-dimensional bounded continuous search space**.\n",
        "- Particles collaborate to discover the optimal solution (i.e., the hyperparameter combination that minimizes a loss function).\n",
        "\n",
        "Each particle \\( i \\) is defined by three key vectors at each iteration \\( t \\):\n",
        "\n",
        "- **Position**: $\\mathbf{x}_i(t) \\in \\mathbb{R}^D $ — current location of particle \\( i \\)\n",
        "- **Velocity**: $mathbf{v}_i(t) \\in \\mathbb{R}^D $ — the movement direction and speed of particle \\( i \\)\n",
        "- **Best Position**: $ \\mathbf{b}_i(t) \\in \\mathbb{R}^D $— the best position found so far by particle \\( i \\) based on the fitness (e.g., validation loss)\n",
        "\n",
        "### Position Update Rule\n",
        "\n",
        "The new position is computed by adding the current velocity to the current position:\n",
        "\n",
        "\\[\n",
        "$\\mathbf{x}_i(t + 1) = \\mathbf{x}_i(t) + \\mathbf{v}_i(t)$\n",
        "\\]\n",
        "\n",
        "### Velocity Update Rule (Conceptual Overview)\n",
        "\n",
        "The velocity is influenced by:\n",
        "- **Cognitive component**: Particle's own best-known position\n",
        "- **Social component**: Best-known position among all particles (global best)\n",
        "\n",
        "The combination of these two attractions causes particles to explore and exploit the search space, ideally converging on an optimal or near-optimal solution over iterations.\n"
      ],
      "metadata": {
        "id": "WVVbnrHNkmYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random as rd\n",
        "\n",
        "best_position_container:list = [] # Keeps track of best position g(t) visited by any particple up to itteration t\n",
        "\n",
        "# 1. Random initialization of particles/subsets of variants for hyperparameters (e.g., Learning Rate;\tOptimizer; Loss Functios)\n",
        "up_lr:float = 0.1 # Upper range of learning rate - initially\n",
        "low_lr:float = 0.01 # Lower range of learning rate\n",
        "\n",
        "optimizers:list = [ # Available optimizers in torch\n",
        "    \"Adadelta\",\n",
        "    \"Adafactor\",\n",
        "    \"Adagrad\",\n",
        "    \"Adam\",\n",
        "    \"AdamW\",\n",
        "    \"SparseAdam\",\n",
        "    \"Adamax\",\n",
        "    \"ASGD\",\n",
        "    \"LBFGS\",\n",
        "    \"NAdam\",\n",
        "    \"RAdam\",\n",
        "    \"RMSprop\",\n",
        "    \"Rprop\",\n",
        "    \"SGD\"\n",
        "]\n",
        "\n",
        "loss_functions:list = [\n",
        "    \"L1Loss\",\n",
        "    \"MSELoss\",\n",
        "    \"CrossEntropyLoss\",\n",
        "    \"CTCLoss\",\n",
        "    \"NLLLoss\",\n",
        "    \"PoissonNLLLoss\",\n",
        "    \"GaussianNLLLoss\",\n",
        "    \"KLDivLoss\",\n",
        "    \"BCELoss\",\n",
        "    \"BCEWithLogitsLoss\",\n",
        "    \"MarginRankingLoss\",\n",
        "    \"HingeEmbeddingLoss\",\n",
        "    \"MultiLabelMarginLoss\",\n",
        "    \"HuberLoss\",\n",
        "    \"SmoothL1Loss\",\n",
        "    \"SoftMarginLoss\",\n",
        "    \"MultiLabelSoftMarginLoss\",\n",
        "    \"CosineEmbeddingLoss\",\n",
        "    \"MultiMarginLoss\",\n",
        "    \"TripletMarginLoss\",\n",
        "    \"TripletMarginWithDistanceLoss\"\n",
        "]\n",
        "\n",
        "\n",
        "hyperparameters_grid:dict = {\n",
        "    'Learning_rate': [0.1, 0.01, 0.005, 0.0010, 0.00020], # rd.uniform(low_lr, up_lr),\n",
        "    'Optimizer': optimizers, # optimizers[rd.randint(0, len(optimizers)-1)],\n",
        "    'Loss Function': loss_functions # loss_functions[rd.randint(0, len(loss_functions)-1)]\n",
        "}"
      ],
      "metadata": {
        "id": "mhdRw6EhmS-O"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Particle initialization/estimator - Method"
      ],
      "metadata": {
        "id": "VKXUMi_U3AKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _init_Populaton():\n",
        "\n",
        "  hyperparameters_grid:dict = {\n",
        "    'Learning_rate': [0.1, 0.01, 0.005, 0.0010, 0.00020],\n",
        "    'Optimizer': optimizers,\n",
        "    'Loss Function': loss_functions\n",
        "}\n",
        "  return hyperparameters_grid\n",
        "\n",
        "# Usage example - Initialize a population of 10 particles/individuals:\n",
        "# for x in range(10):\n",
        "#     particle = init_Populaton()\n",
        "#     print(particle)"
      ],
      "metadata": {
        "id": "yDEMu8GIuUiM"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate fitness - Method"
      ],
      "metadata": {
        "id": "J4fWaZfn-QRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_fitness(self,\n",
        "                     X_train,\n",
        "                     X_test,\n",
        "                     y_train,\n",
        "                     y_test,\n",
        "                     hyperparameters):\n",
        "        \"\"\"\n",
        "        Evaluate the fitness of a set of hyperparameters.\n",
        "\n",
        "        Parameters:\n",
        "            - estimator: The estimator object.\n",
        "            - X_train: Training features.\n",
        "            - X_test: Testing features.\n",
        "            - y_train: Training labels.\n",
        "            - y_test: Testing labels.\n",
        "            - hyperparameters: The set of hyperparameters to evaluate.\n",
        "\n",
        "        Returns:\n",
        "            - score: The accuracy score of the estimator with the given hyperparameters.\n",
        "        \"\"\"\n",
        "        # Unpack hyperparameters\n",
        "        estimator_instance = self._create_estimator(hyperparameters)\n",
        "\n",
        "        estimator_instance.fit(X_train, y_train)\n",
        "        y_pred = estimator_instance.predict(X_test)\n",
        "        accuracy_pso = accuracy_score(y_test, y_pred)\n",
        "        return accuracy_pso"
      ],
      "metadata": {
        "id": "b5hYBod5-TuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the searching space - Method"
      ],
      "metadata": {
        "id": "eRSDlZPc3CTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pso_hyperparameter_optimization(self,\n",
        "                                    X_train,\n",
        "                                    X_test,\n",
        "                                    y_train,\n",
        "                                    y_test,\n",
        "                                    num_particles,\n",
        "                                    num_iterations,\n",
        "                                    c1 = 2.05,\n",
        "                                    c2 = 2.05,\n",
        "                                    num_jobs=-1,\n",
        "                                    w=0.72984):\n",
        "        \"\"\"\n",
        "        Perform hyperparameter optimization using Particle Swarm Optimization (PSO).\n",
        "\n",
        "        Parameters:\n",
        "            - estimator: The estimator object (e.g., KNeighborsClassifier, ViT).\n",
        "            - data: The dataset.\n",
        "            - target_column_index: Index of the target column in the dataset.\n",
        "            - num_particles: Number of particles in the population.\n",
        "            - num_iterations: Number of iterations for the PSO algorithm.\n",
        "            - c1: Acceleration constant. Default value is c1 = 2.05\n",
        "            - c2: Acceleration constant. Default value is c2 = 2.05\n",
        "            - num_jobs: Number of parallel jobs for fitness evaluation.\n",
        "            - inertia weight: Inertia constant. Default value is w=0.72984 according to the paper by M. Clerc and J. Kennedy\n",
        "\n",
        "        Returns:\n",
        "            - global_best_position: The best set of hyperparameters found.\n",
        "            - global_best_fitness: The best accuracy found.\n",
        "        \"\"\"\n",
        "        if self.random_seed is not None:\n",
        "            np.random.seed(self.random_seed)\n",
        "\n",
        "        # 1. Initialize the population of particles\n",
        "        hyperparameter_space = self._init_Populaton()\n",
        "        progress_bar = tqdm(total=num_iterations, desc=\"PSO Progress\")\n",
        "        population:list = [] # Container 1: Population\n",
        "\n",
        "        for _ in range(self.num_particles):\n",
        "            hyperparameters = [np.random.choice(hyperparameter_space[param]) for param in hyperparameter_space]\n",
        "            population.append(hyperparameters)\n",
        "\n",
        "\n",
        "        # 2. Initialize velocity and best position\n",
        "        velocity = [[0] * len(hyperparameter_space) for _ in range(num_particles)] # Container 2: Velocity, of each hyperparameter-set stays zero at initialization phase\n",
        "        best_position = population.copy() # Container 3: Best position do not differ from population\n",
        "        global_best_fitness = -float(\"inf\") # Conainer 4: Best positions are all the same across individuals hence fitness stays constant\n",
        "        global_best_position = [] # Container 5: Best position a given individual has ever reached\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # PSO optimization loop\n",
        "        for _ in range(num_iterations):\n",
        "            fitness = Parallel(n_jobs=num_jobs)(\n",
        "                delayed(self.evaluate_fitness)(X_train, X_test, y_train, y_test, particle)\n",
        "                for particle in population\n",
        "            )\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hrsR8TYB3K9Z"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "Qw6m0Dh0Y-AD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_score"
      ],
      "metadata": {
        "id": "Fq7ibJy5Y_cS"
      },
      "execution_count": 47,
      "outputs": []
    }
  ]
}